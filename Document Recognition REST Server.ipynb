{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalation dependncies: \n",
    "- pip install Flask\n",
    "- pip instal gevent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from gevent.pywsgi import WSGIServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('spanish'))\n",
    "lemmatization_script = r'C:\\Document Recognition REST Server\\lemmatization-es.txt'\n",
    "path_tfidf = r'C:\\Document Recognition REST Server\\tfidf.pickle'\n",
    "\n",
    "\n",
    "def clean_1(df):\n",
    "    disclaimer = r'(?=\"En relación)(.*?)(\\)\")' # nota prawna na dole dokumentu\n",
    "    df['Content1'] = df['attachment_body'].str.replace(\"\\r\", \" \")\n",
    "    df['Content1'] = df['Content1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content1'] = df['Content1'].str.replace(disclaimer, \"\")\n",
    "    df['Content1'] = df['Content1'].str.replace('\"', \"\")\n",
    "    df['Content1'] = df['Content1'].str.replace('/', \" \")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_2(df):\n",
    "    df['Content2'] = df['Content1'].str.lower()\n",
    "    df['Content2'] = df['Content2'].str.replace('tlf', \"teléfono\")\n",
    "    df['Content2'] = df['Content2'].str.replace(':', \" \")\n",
    "    df['Content2'] = df['Content2'].str.replace(',', \" \")\n",
    "    df['Content2'] = df['Content2'].str.replace('correo electrónico:', \"email\")\n",
    "    df['Content2'] = df['Content2'].str.replace('jdo', \"juzgado\")\n",
    "    df['Content2'] = df['Content2'].str.replace('xdo', \"juzgado\")\n",
    "    df['Content2'] = df['Content2'].str.replace('upad ', \"juzgado \")\n",
    "    df['Content2'] = df['Content2'].str.replace('upad', \"juzgado\")\n",
    "    df['Content2'] = df['Content2'].str.replace('j.primera', \"juzgado primera\")\n",
    "    df['Content2'] = df['Content2'].str.replace('j. primera', \"juzgado primera\")\n",
    "    df['Content2'] = df['Content2'].str.replace('inst.', \"instancia \", regex=False)\n",
    "    df['Content2'] = df['Content2'].str.replace('n.i.g.', \"nig\")\n",
    "    df['Content2'] = df['Content2'].str.replace('d.a.c.', \"dac\")\n",
    "    df['Content2'] = df['Content2'].str.replace('l.e.c.', \"lec\")\n",
    "    df['Content2'] = df['Content2'].str.replace('ª', \"a\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_3(df):\n",
    "    pattern = r\"\\b[a-zA-Z]\\b\" #pojedyncze litery\n",
    "    punctuation_signs = list(\"?!.;%&+\") # bedziemy usuwac znaki specjalne\n",
    "    df['Content3'] = df['Content2'].str.replace(pattern, \"\")\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content3'] = df['Content3'].str.replace(punct_sign, ' ')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_4(df):\n",
    "    pattern = r\"(?=código)(.*?)(?=página)\" #notka prawna na dole dokumentu\n",
    "    df['Content4'] = df['Content3'].str.replace(pattern, \"\")\n",
    "    df['Content4'] = df['Content4'].str.replace(\"(\", \"\")\n",
    "    df['Content4'] = df['Content4'].str.replace(\")\", \"\")\n",
    "    df['Content4'] = df['Content4'].str.replace(\"-\", \"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stop_words(df, stop_words):\n",
    "    df['Content5'] = df['Content4']    \n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content5'] = df['Content5'].str.replace(regex_stopword, '')\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_digits(df):\n",
    "    df['Content6'] = df['Content5'].str.replace('\\d+', '')\n",
    "    return df\n",
    "\n",
    "\n",
    "def lemmatize(word, lemmaDict):\n",
    "    return lemmaDict.get(word, word)\n",
    "\n",
    "\n",
    "def lemmatize_df(df):\n",
    "    lemmaDict = {}\n",
    "    with open(lemmatization_script, 'rb') as f:\n",
    "        data = f.read().decode('utf8').replace(u'\\r', u'').split(u'\\n')\n",
    "        data = [a.split(u'\\t') for a in data]\n",
    "        data[0][0] = '1'\n",
    "        \n",
    "    for a in data:\n",
    "        if len(a) >1:\n",
    "            lemmaDict[a[1]] = a[0]\n",
    "    \n",
    "    df['Content7'] = df['Content6'].map(lambda x: \" \".join(x.split()))\n",
    "    df['Content7'] = df['Content7'].str.split(\" \").tolist()\n",
    "    df['Content8'] = df['Content7'].map(lambda x: [lemmatize(word, lemmaDict) for word in x])\n",
    "    df['Content8'] = df['Content8'].map(lambda x: \" \".join(x))    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean(df):\n",
    "    pd_request = {'attachment_body': [df]}\n",
    "    df = pd.DataFrame(pd_request)\n",
    "    df = clean_1(df)\n",
    "    df = clean_2(df)\n",
    "    df = clean_3(df)\n",
    "    df = clean_4(df)\n",
    "    df = remove_stop_words(df, stop_words)\n",
    "    df = remove_digits(df)\n",
    "    df = lemmatize_df(df)\n",
    "    return df\n",
    "\n",
    "def discretization(df):\n",
    "    df = clean(df)\n",
    "    with open(path_tfidf, 'rb') as data:\n",
    "        tfidf = pickle.load(data)\n",
    "    return tfidf.transform(df['Content8']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "\n",
    "path_model = r'C:\\Document Recognition REST Server\\RFC-md4-ne70.pickle'\n",
    "\n",
    "\n",
    "def model_prediction(parsed_content):\n",
    "    with open(path_model, 'rb') as data:\n",
    "        model = pickle.load(data)\n",
    "\n",
    "    prediction = model.predict(parsed_content)\n",
    "    pred_proba = model.predict_proba(parsed_content)\n",
    "\n",
    "    output = [prediction[0], pred_proba[0][prediction[0]]]\n",
    "    output = ';'.join(str(v) for v in output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExecutePythonScript(path_to_script, arguments):\n",
    "    try:\n",
    "        return subprocess.check_output([sys.executable, path_to_script, arguments])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(\"command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output))\n",
    "        return e.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_script = r'C:\\Document Recognition REST Server\\CleanAndLemmatize.py'\n",
    "mlmodel_script = r'C:\\Document Recognition REST Server\\MLmodel.py'\n",
    "test_script = r'C:\\Document Recognition REST Server\\Test.py'\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Welcome on ML Server\"        \n",
    "\n",
    "@app.route('/test', methods=['POST'])\n",
    "def test():\n",
    "    \n",
    "    # work with input \n",
    "    \n",
    "    body = request.data.decode(\"utf-8\")\n",
    "    print(\"body: \", body)\n",
    "    \n",
    "    print (request.is_json)\n",
    "    content = request.get_json()\n",
    "    print (content)\n",
    "    \n",
    "    # work with script\n",
    "    \n",
    "    output = ExecutePythonScript(test_script, body).decode(\"utf-8\")\n",
    "    \n",
    "    # Work with output\n",
    "    \n",
    "    o_data = {\n",
    "        'pred'  : 0.9,\n",
    "        'confidence' : 0.8\n",
    "    }\n",
    "    resp = jsonify(o_data)\n",
    "    resp.status_code = 200\n",
    "    \n",
    "    print(\"response: \", resp)\n",
    "    return resp\n",
    "\n",
    "\n",
    "@app.route('/m', methods=['POST'])\n",
    "def run_model():\n",
    "    pdf_text = request.data.decode(\"ansi\")\n",
    "    \n",
    "    input_to_model = discretization(pdf_text)\n",
    "    output = model_prediction(input_to_model)\n",
    "    \n",
    "    #input_to_model = ExecutePythonScript(clean_script, pdf_text)\n",
    "    #output = ExecutePythonScript(mlmodel_script, \"b\").decode(\"utf-8\")\n",
    "    \n",
    "    print(\"output: \", output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body:  \n",
      "False\n",
      "None\n",
      "response:  <Response 30 bytes [200 OK]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "::1 - - [2019-08-02 18:52:46] \"POST /test HTTP/1.1\" 200 138 0.078146\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    http_server = WSGIServer(('', 5000), app)\n",
    "    http_server.serve_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
